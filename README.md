# Building-a-Lexicon-for-Financial-Domain
- Find relations between terms using Text Mining 
- Identify important terms from Financial News
- Extract definitions of these terms from online sources
- Find relations between terms using Text Mining
- Implement a query extension mechanism using the model
- Used Lucene java library
Approaches:
  1. Indexing
  Since building a search engine taking it from the top is quite expensive, we wanted to use open source libraries to benefit and         considered to use Lucene. Lucene is an open source library improved by Apache to carry out full text search processes. It indexes data and  make a search in indexed data.
  During indexing, terms and definitions are stored as “filename” and “contents”. After specifying fields, we declared Indexwriter to analyze documents and store them into directory. Process of indexing is shown below.
  
  2.  Searching
  For searching, our purpose was to find documents where query term occurred. Our search engine functions are starting with taking query, parse and convert it to query object with Query Parser to make the query understandable by Index Searcher. Index Searcher accesses to Index Reader and searches the query in indexed corpus. Indexed corpus returns TopDocs which includes Hits. Hits contains all the documents which are found in field “contents”. Additionally Lucene offers many type of analyzer to parse query. For our project, Standard Analyzer of Lucene library was best function that we can use besides Simple Analyzer, Whitespace Analyzer methods. Difference of Standard Analyzer is that it makes the token lowercase, remove common words and punctuation.
In natural language processing, while one word refers to one meaning, when it occurs together with a different words, it refers to different meaning. During process, they need to be observed in terms of semantics. For our case, we observed financial terms occurrence in corpus to prevent disambiguation and wanted to get clear for each financial term occurs regarding to finance field in corpus. Our aim was to make calculations and and evaluation accurately. Therefore we needed to avoid parsing each financial term query that we searched in the corpus. As an example, while “Finance” has one meaning as a word, collocation of “Finance Charge” has different meaning, but they are still related in respect to same semantics. In general, all financial terms represent a knowledge by gathering under the finance field. Since many terms are related to each other, we searched each financial term in corpus and created a output document for each query that contains other financial terms to show those terms as inner link and their occurrence in documents may be related to term document. In investopedia related terms are restricted with 6 terms, we wanted to pointed out concept linking for the rest of the related terms which investopedia doesn’t display.
  3. Measures of Term Relatedness
  One of the purposes of the search engine is to show more information than results of user’s query. For instance, many websites display “related terms” section which includes many terms considered as related to query term. It is considered as user can pay attention other topics or she/he can extends her/his search to get more related information with respect to relevance topic.Website of Investopedia includes relatedness section in a template to give more information about existing topic on the page. They show totally 6 related terms for each financial term in the website. As we don’t know which algorithm or methods are used by Investopedia developers, we would like to specify our related terms based on some calculations. To obtain this relatedness, there are some similarity measures which are running behind to find resemblance. These measures are based on similarity between documents where query term occurred. We used two methods to find relatedness among terms. For each financial term, our goal was to calculate similarity against rest of the terms and obtain their scores along with them. As a result of this process, results were ranked and stored in object files which were taking their names from query term. Two types of method were computed among financial terms.
    
    Normalized Pointwise-Mutual Information
    Distributional Semantics is an area which observes and categorizes entities based on their distributional properties in large corpus. Term co-occurrence in corpus provides us to discover term relatedness. Distributional semantics contains 5 types of modelling. However we are only interested in frequency weighting. Our aim was to find relatedness among terms and create a distributional vector space to calculate Cosine Similarity with vector of each query term. Vector space model that we build was used to calculate Cosine similarity. Mutual Information is measure to calculate amount of information in common between two distributions in the same dimension. It assumes that one event gives information about other events. In contrast to Mutual Information, in text mining process, Pointwise Mutual Information is used to measure relatedness of two words by calculating probability of their occurrence in corpus. With Pointwise Mutual Information, relatedness of two terms is measured by using web queries. If calculation gives a high score, alongside of they are strongly related; it also shows these two terms tend to occur together in corpus.
    
    Cosine Similarity
    Cosine Similarity is used to measure similarity between two words or documents. It is one of the most used similarity measure. Words are represented as vectors and the angle between vectors refers to Cosine similarity and shows how these two words are similar. Cosine similarity between two vectors that shows exactly the same side will be 1 (cos(0) = 1). It obtains that these two vectors are exactly the same. Cosine similarity between two vectors that are completely different will be 0 (cos(90) = 0). It indicates decorrelation (orthogonal) between these two vectors. Additionally Cosine similarity between two vectors which are poles apart and directly away will be -1 (cos(180) = -1). It displays opposition between two vectors. To calculate Cosine similarity, token has to be deduced from the documents. There are many ways to create vectors. We benefitted from NPMI values to create vectors and let them represent each financial term in the same dimension. We only considered positive values to evaluate them into vector length.
    
    
    With Pointwise Mutual Information, we stored output values for each financial term in documents. Our next step was to create bags of concepts. Bag of concepts is used in Natural Language Processing and Information Retrieval. Each document are represented with its words. Instead of represent all the term in each document, we built [term x term] matrix and put NPIM values to each corresponding cell. Aim was to store these key values corresponding to each financial term in matrix. Each row in matrix represents term vector that is used for Cosine Similarity calculation.
    
    
    Query Expansion
    The main aim of an Information Retrieval system is to access relevant documents in the collection and sort out irrelevant documents in order to meet information need for users. Search engine is one of Information Retrieval system to reach information. Search engine is a mechanism used to access information on the internet . It is composed of three components: Web engine, search index and user interface. After entering the query, search engine ranks results according to their hit rates. Web engine crawls website contents according to their hub connections. These contents are indexed and made ready to be searched. User interface provides these contents to be visible according to user’s query. While search engine’s task show the relevant results according to query, their other functionality is to collect further information to show. For that search engine expand the query and it is called as query expansion. Query Expansion is that to redevelop the seed query to match more documents in Information Retrieval operations. It adds more term (keywords) to query and the aim is to improve Precision and Recall. As an example of user's query: car can be expanded such as cars, automobile, auto. In general there two options to improve results of query. First is local methods. Local methods based on relevance feedback from the user and Pseudo relevance feedback (Relevance Model). In relevance feedback , user feedback on relevance of documents. User marks some results as they are relevant or non-relevant and system evaluate these feedback to improve results. But it can be inefficient since user may not cover all corpus very well to give relevance feedback. There are some algorithms to improve relevance feedback such as calculations based on weighting, Rocchio algorithm Second is global methods. It is based on Query Expansion methods such as focused on thesauri, automatic thesaurus generation. In thesauri method , for each term in a query, synonyms and related words of query term from the corpus are retrieved and applied to search. It is generally increases Recall but can decrease Precision. For instance, query term “interest rate” can be expanded as “interest fascinate rate evaluate” and it triggers an evaluation which includes a query with many ambiguous terms. Automatic thesauri generation essays to generate thesaurus automatically after analyzing the documents in corpus. It is based on similarity between two words. According to method of automatic thesaurus generation: firstly, two words are similar, if they co-occur with similar words, secondly, two words are similar if they occur in a given grammatical relation with same words. Best way to compute a co-occurrence thesaurus is to create [term x term] matrix. This is how we did our query expansion by taking an advantage of [term x term] matrix.
     1. Query Expansion 1: For each financial term that is entered to search as a query, we retrieved top 3 related terms of query term, created new query consists of old query and its related terms. Then we ranked results. We considered to use related terms that are composed of NPMI calculation. As a result, we were not satisfied with this method since we don’t know whether sorting result of the related terms is correct. According to query search, most related documents has to be ranked firstly. This method didn’t meet our expectation in sorting, we wanted to follow second method to get more control on us and make the query search efficient.
     2. Query Expansion 2: Second method was retrieved first 3 related terms of query and their NPMI values. As we know after the search, Lucene sorts results according to their scores. Since we know it, we wanted to be sure control is more on us instead of Lucene. Based on scenario for query, firstly query was entered, secondly 3 top related terms were taken from the relevant object file of query term which keeps NPMI values and related terms. Thirdly instead of creating a new query, we created 4 different queries and searched each query in the corpus. First query was initial query which user wanted to search. Other three queries are 3 top related terms of query. At the beginning initial query was searched and results were ranked. For other 3 queries, searching was applied separately and NPMI values of each related term were multiplied by each hit score and results were ranked according to new scores.
    
